{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 13:57:30,169 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 28 document splits...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd_2025-10-28_14-03-52.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Retriever] Retrieved 4 documents.\n",
      "[Generator] Generating answer...\n",
      "Final answer: The purpose of the project according to the PRD (Product Requirements Document) is to revolutionize the new hire experience from day one by transforming a typically fragmented and overwhelming process into a structured, engaging, and efficient journey. The platform aims to provide personalized learning paths, streamline administrative tasks, and foster quicker team integration for new employees, while offering HR and managers critical tools to track progress and provide timely support. The vision is to empower every new hire to become a productive and engaged contributor faster, significantly reducing ramp-up time and enhancing overall employee retention and satisfaction.\n"
     ]
    }
   ],
   "source": [
    "# Single-Agent RAG Graph (Challenge 1)\n",
    "from typing import TypedDict, List, Optional\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from utils import get_completion\n",
    "\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    grade: str # add for future challenge\n",
    "\n",
    "# --- Node Definitions ---\n",
    "# Retriever node: pulls relevant documents for the question\n",
    "def retrieve(state: AgentState) -> AgentState:\n",
    "    question = state[\"question\"]\n",
    "    if not question:\n",
    "        return {}\n",
    "    if retriever is None:\n",
    "        print(\"[Retriever] No retriever available.\")\n",
    "        return {}\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"[Retriever] Retrieved {len(docs)} documents.\")\n",
    "    return {\"documents\": docs, \"question\": question} # include original question, not strictly required as API maintains state object throughout edge transitions\n",
    "\n",
    "# Generator node: builds a prompt from question + docs and calls LLM\n",
    "def generate(state):\n",
    "    print(\"[Generator] Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer, \"question\": question, \"documents\": documents} # include previous step's states\n",
    "\n",
    "# --- Build Graph ---\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"RETRIEVE\", retrieve)\n",
    "graph.add_node(\"GENERATE\", generate)\n",
    "graph.add_edge(START, \"RETRIEVE\")\n",
    "graph.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "graph.add_edge(\"GENERATE\", END)\n",
    "app = graph.compile()\n",
    "\n",
    "# --- Invoke Graph ---\n",
    "inputs = {\"question\": \"What is the purpose of this project according to the PRD?\"}\n",
    "result = app.invoke(inputs)\n",
    "print(f\"Final answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking Grader Graph with a relevant question ---\n",
      "[Retriever] Retrieved 4 documents.\n",
      "[Grader] Grading the answer...\n",
      "[Grader] Grade: no.\n",
      "Final Answer: Could not answer question.\n",
      "\n",
      "--- Invoking Grader Graph with an irrelevant question ---\n",
      "[Retriever] Retrieved 4 documents.\n",
      "[Grader] Grading the answer...\n",
      "[Grader] Grade: no.\n",
      "Final Answer: Could not answer question.\n"
     ]
    }
   ],
   "source": [
    "def grader(state: AgentState) -> AgentState:\n",
    "    print(\"[Grader] Grading the answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are a grader. Given the question and the provided documents, determine if the documents provide sufficient information and context to answer the question. Respond with 'yes' or 'no'.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
    "    grade = llm.invoke(prompt).content.strip().lower()\n",
    "    print(f\"[Grader] Grade: {grade}\")\n",
    "    return {\"grade\": grade}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    if state[\"grade\"].lower() == \"yes\":\n",
    "        print(\"DECISION: Documents are relevant. Proceed to generation.\")\n",
    "        return \"GENERATE\"\n",
    "    else:\n",
    "        return END\n",
    "        return END\n",
    "    \n",
    "\n",
    "workflow_v2 = StateGraph(AgentState)\n",
    "workflow_v2.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v2.add_node(\"GRADE\", grader)\n",
    "workflow_v2.add_node(\"GENERATE\", generate)\n",
    "\n",
    "workflow_v2.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v2.add_edge(\"RETRIEVE\", \"GRADE\")\n",
    "workflow_v2.add_conditional_edges(\"GRADE\", decide_to_generate)\n",
    "workflow_v2.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v2 = workflow_v2.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with a relevant question ---\")\n",
    "inputs = {\"question\": \"What database schema will we use?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with an irrelevant question ---\")\n",
    "inputs = {\"question\": \"What is the weather in Paris?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 14 document splits...\n",
      "Creating vector store from 14 document splits...\n",
      "\n",
      "--- Invoking Research Team with a PRD question ---\n",
      "---NODE: PROJECT MANAGER (ROUTER)---\n",
      "PM Decision: Route to PRD_RESEARCHER\n",
      "---NODE: PRD RESEARCHER---\n",
      "---NODE: SYNTHESIZE ANSWER---\n",
      "Final Answer: The main user personas for this application are:\n",
      "\n",
      "1. **The Eager Contributor (New Hire):** A new employee, like Sarah, who is eager to start but feels overwhelmed by the amount of information, struggles to determine what to learn first, and faces difficulties in accessing company policies and finding help.\n",
      "\n",
      "2. **The Onboarding Orchestrator (HR Onboarding Specialist):** An HR specialist, like Mark, who spends a lot of time manually tracking down new hires for incomplete forms and repetitively answering questions, and lacks tools to efficiently track and support new hires’ progress, leading to compliance risks.\n",
      "\n",
      "3. **The Team Integrator (Engineering Team Lead):** While specific scenarios for this persona aren’t detailed in the provided documents, they generally focus on integrating new hires into existing teams.\n",
      "\n",
      "These personas highlight the application's goal to streamline onboarding and improve efficiency for both new hires and HR professionals.\n",
      "\n",
      "--- Invoking Research Team with a technical question ---\n",
      "---NODE: PROJECT MANAGER (ROUTER)---\n",
      "PM Decision: Route to TECH_RESEARCHER\n",
      "---NODE: TECH RESEARCHER---\n",
      "---NODE: SYNTHESIZE ANSWER---\n",
      "Final Answer: The columns in the users table are:\n",
      "\n",
      "- id: INTEGER PRIMARY KEY\n",
      "- full_name: TEXT NOT NULL\n",
      "- email: TEXT NOT NULL UNIQUE\n",
      "- user_type: TEXT NOT NULL CHECK (user_type IN ('new_hire', 'hr_specialist', 'manager'))\n",
      "- role_id: INTEGER\n",
      "- manager_id: INTEGER\n",
      "- mentor_id: INTEGER\n",
      "- start_date: TEXT\n",
      "- created_at: TEXT NOT NULL DEFAULT (strftime('%Y-%m-%dT%H:%M:%fZ', 'now'))\n",
      "- updated_at: TEXT NOT NULL DEFAULT (strftime('%Y-%m-%dT%H:%M:%fZ', 'now'))\n",
      "\n",
      "Additionally, there are foreign key constraints on role_id, manager_id, and mentor_id referencing other tables.\n"
     ]
    }
   ],
   "source": [
    "prd_retriever = create_knowledge_base([\"artifacts/day1_prd_2025-10-28_14-03-52.md\"])\n",
    "tech_retriever = create_knowledge_base([\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"])\n",
    "\n",
    "class ResearchTeamState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# 2. Define the agent nodes\n",
    "def prd_researcher(state):\n",
    "    print(\"---NODE: PRD RESEARCHER---\")\n",
    "    documents = prd_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def tech_researcher(state):\n",
    "    print(\"---NODE: TECH RESEARCHER---\")\n",
    "    documents = tech_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def synthesize_answer(state):\n",
    "    print(\"---NODE: SYNTHESIZE ANSWER---\")\n",
    "    prompt = f\"Based on the following documents, create a concise answer to the user's question.\\n\\nQuestion: {state['question']}\\n\\nDocuments: {state['documents']}\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def project_manager_router(state):\n",
    "    print(\"---NODE: PROJECT MANAGER (ROUTER)---\")\n",
    "    prompt = f\"You are a project manager. Based on the user's question, should you route this to the PRD expert or the Technical expert? Answer with 'PRD_RESEARCHER' or 'TECH_RESEARCHER'.\\n\\nQuestion: {state['question']}\"\n",
    "    decision = llm.invoke(prompt).content\n",
    "    print(f\"PM Decision: Route to {decision}\")\n",
    "    if 'PRD_RESEARCHER' in decision:\n",
    "        return \"PRD_RESEARCHER\"\n",
    "    else:\n",
    "        return \"TECH_RESEARCHER\"\n",
    "\n",
    "# 3. Build the graph\n",
    "workflow_v3 = StateGraph(ResearchTeamState)\n",
    "workflow_v3.add_node(\"PRD_RESEARCHER\", prd_researcher)\n",
    "workflow_v3.add_node(\"TECH_RESEARCHER\", tech_researcher)\n",
    "workflow_v3.add_node(\"SYNTHESIZE\", synthesize_answer)\n",
    "\n",
    "workflow_v3.add_conditional_edges(\"__start__\", project_manager_router)\n",
    "workflow_v3.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"SYNTHESIZE\", END)\n",
    "\n",
    "app_v3 = workflow_v3.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a PRD question ---\")\n",
    "inputs = {\"question\": \"What are the main user personas for this application?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a technical question ---\")\n",
    "inputs = {\"question\": \"What columns are in the users table?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

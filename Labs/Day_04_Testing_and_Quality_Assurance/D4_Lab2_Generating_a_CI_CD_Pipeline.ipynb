{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 14:50:57,476 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "fastapi\n",
      "sqlalchemy\n",
      "uvicorn\n",
      "pydantic\n",
      "pydantic[email]\n",
      "pytest\n"
     ]
    }
   ],
   "source": [
    "requirements_prompt = f\"\"\"\n",
    "Generate a requirements.txt file for a Python project based on the following application code:\n",
    "\n",
    "Application Code:\n",
    "{app_code}\n",
    "\n",
    "The requirements.txt file should list all necessary Python packages and their versions needed to run the application.\n",
    "Analyze the code for import statements and generate a list of all external dependencies.\n",
    "**Include pytest, pydantic, and pydantic[email] for testing.** \n",
    "\n",
    "Generate properly formatted requirements.txt content only, without any additional explanations or text.\n",
    "Do not include specific version numbers for the packages.\n",
    "**ONLY include the content of the requirements.txt file in your response.\n",
    "Do not include any extra text, explainations, or formatting. Do not start with \"plaintext\"**\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"app/requirements.txt\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "# Use the official Python image as a parent image\n",
      "FROM python:3.11-slim AS builder\n",
      "\n",
      "# Set the working directory in the container\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the requirements file into the container\n",
      "COPY requirements.txt .\n",
      "\n",
      "# Install any needed packages specified in requirements.txt\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Use a second FROM statement to start a new stage\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set the working directory in the container\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the dependencies from the builder stage\n",
      "COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n",
      "COPY --from=builder /usr/local/bin /usr/local/bin\n",
      "\n",
      "# Copy the current directory contents into the container at /app\n",
      "COPY . .\n",
      "\n",
      "# Make port 80 available to the world outside this container\n",
      "EXPOSE 80\n",
      "\n",
      "# Run app.py when the container launches\n",
      "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/labadmin/Desktop/Workspace/AG-AISOFTDEV/artifacts/app/Dockerfile')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dockerfile_prompt = f\"\"\"\n",
    "Generate a multi-stage Dockerfile for a Python FastAPI application.\n",
    "\n",
    "The Dockerfile should follow these specifications:\n",
    "1. Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "2. In the build stage, install dependencies from a requirements.txt file.\n",
    "3. Copy the installed dependencies and the application code to the final stage.\n",
    "4. The `CMD` should execute the application using `uvicorn`, assuming the main application file is located at `app/main.py` and the FastAPI app instance is named `app`.\n",
    "\n",
    "Generate only the Dockerfile content without any additional explanations or text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "\n",
    "print(cleaned_dockerfile)\n",
    "save_artifact(cleaned_dockerfile, \"app/Dockerfile\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GitHub Actions Workflow ---\n",
      "name: CI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches:\n",
      "      - main\n",
      "  pull_request:\n",
      "    branches:\n",
      "      - main\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "\n",
      "    steps:\n",
      "    - name: Check out repository code\n",
      "      uses: actions/checkout@v2\n",
      "\n",
      "    - name: Set up Python 3.11\n",
      "      uses: actions/setup-python@v2\n",
      "      with:\n",
      "        python-version: '3.11'\n",
      "\n",
      "    - name: Install dependencies\n",
      "      run: |\n",
      "        python -m pip install --upgrade pip\n",
      "        pip install -r requirements.txt\n",
      "\n",
      "    - name: Run tests with pytest\n",
      "      run: |\n",
      "        pytest tests\n"
     ]
    }
   ],
   "source": [
    "ci_workflow_prompt = \"\"\"\n",
    "Create a GitHub Actions workflow file (.github/workflows/ci.yml) for a Python FastAPI application.\n",
    "There should be a single job named 'build-and-test' that runs on 'ubuntu-latest'.\n",
    "\n",
    "The job should include the following steps:\n",
    "1. Checkout the repository code.\n",
    "2. Set up a Python environment with version 3.11.\n",
    "3. Install dependencies from a requirements.txt file located at the root of the repository.\n",
    "4. Run tests using pytest, assuming tests are located in a 'tests' directory.\n",
    "\n",
    "The workflow should be triggered on pushes and pull requests to the main branch.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Note: The save_artifact helper creates directories as needed.\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

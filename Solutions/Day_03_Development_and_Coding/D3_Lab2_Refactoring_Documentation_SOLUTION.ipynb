{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Lab 2: Refactoring & Documentation (Solution)\n",
    "\n",
    "**Objective:** Use an LLM to refactor a complex Python function to improve its readability and maintainability, and then generate comprehensive, high-quality documentation for the project.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts for the refactoring and documentation lab. It demonstrates how to guide an LLM to perform specific code quality improvements and generate structured documentation from multiple sources.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-21 21:28:52,257 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=deepseek-ai/DeepSeek-V3.1 latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:28:52,502 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:28:52,795 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-5-2025-08-07 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for each task so we can pick recent models from different providers.\n",
    "# - Refactoring: strong instruction-following model\n",
    "# - Docstrings: model tuned for clarity/style\n",
    "# - README generation: high-capacity synthesis model\n",
    "refactor_client, refactor_model_name, refactor_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "doc_client, doc_model_name, doc_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "readme_client, readme_model_name, readme_api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Code to Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_code = \"\"\"\n",
    "def process_data(data, operation):\n",
    "    if operation == 'sum':\n",
    "        total = 0\n",
    "        for i in data:\n",
    "            total += i\n",
    "        return total\n",
    "    elif operation == 'average':\n",
    "        total = 0\n",
    "        for i in data:\n",
    "            total += i\n",
    "        return total / len(data)\n",
    "    elif operation == 'max':\n",
    "        max_val = data[0]\n",
    "        for i in data:\n",
    "            if i > max_val:\n",
    "                max_val = i\n",
    "        return max_val\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Refactoring the Code\n",
    "\n",
    "**Explanation:**\n",
    "This prompt is highly specific about the desired outcome. Instead of just saying \"improve this code,\" we give the LLM concrete principles to follow: apply the 'Single Responsibility Principle,' use built-in functions, and add type hints. This guidance transforms a vague request into a precise engineering task, leading to a much higher-quality output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:28:52,818 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Refactoring Code ---\n",
      "from typing import List, Union, Optional\n",
      "\n",
      "\n",
      "def process_data(data: List[Union[int, float]], operation: str) -> Optional[Union[int, float]]:\n",
      "    operation_handlers = {\n",
      "        'sum': _calculate_sum,\n",
      "        'average': _calculate_average,\n",
      "        'max': _calculate_max\n",
      "    }\n",
      "    \n",
      "    if operation not in operation_handlers:\n",
      "        return None\n",
      "    \n",
      "    return operation_handlers[operation](data)\n",
      "\n",
      "\n",
      "def _calculate_sum(data: List[Union[int, float]]) -> Union[int, float]:\n",
      "    return sum(data)\n",
      "\n",
      "\n",
      "def _calculate_average(data: List[Union[int, float]]) -> float:\n",
      "    return sum(data) / len(data) if data else 0.0\n",
      "\n",
      "\n",
      "def _calculate_max(data: List[Union[int, float]]) -> Optional[Union[int, float]]:\n",
      "    return max(data) if data else None\n"
     ]
    }
   ],
   "source": [
    "refactor_prompt = f\"\"\"\n",
    "You are a senior Python developer who writes clean, efficient, and maintainable code.\n",
    "\n",
    "Please refactor the following Python code. Apply the 'Single Responsibility Principle' by breaking the main function into smaller, more focused functions. Also, use modern Python features like built-in functions and add type hints.\n",
    "\n",
    "**Code to Refactor:**\n",
    "```python\n",
    "{bad_code}\n",
    "```\n",
    "\n",
    "Output only the refactored Python code.\n",
    "\"\"\"\n",
    "\n",
    "# Enhance the prompt for better results and consistency with other labs\n",
    "enhanced_refactor_prompt = prompt_enhancer(refactor_prompt)\n",
    "\n",
    "print(\"--- Refactoring Code ---\")\n",
    "refactored_code = get_completion(enhanced_refactor_prompt, refactor_client, refactor_model_name, refactor_api_provider)\n",
    "cleaned_code = clean_llm_output(refactored_code, language='python')\n",
    "print(cleaned_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Docstrings\n",
    "\n",
    "**Explanation:**\n",
    "This prompt builds on the previous step. We provide the newly refactored code and ask for another specific, structured output: Google-style docstrings. LLMs are exceptionally good at this type of structured text generation. They can parse the function signature to identify the arguments and return types and generate well-formatted, descriptive documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:29:08,299 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Docstrings ---\n",
      "from typing import List, Union, Optional\n",
      "\n",
      "\n",
      "def process_data(data: List[Union[int, float]], operation: str) -> Optional[Union[int, float]]:\n",
      "    \"\"\"Processes a list of numbers using a specified operation.\n",
      "\n",
      "    Args:\n",
      "        data (List[Union[int, float]]): A list of integers or floats to process.\n",
      "        operation (str): The name of the operation to perform ('sum', 'average',\n",
      "            'max').\n",
      "\n",
      "    Returns:\n",
      "        Optional[Union[int, float]]: The result of the operation, or None if the\n",
      "        operation is not supported.\n",
      "    \"\"\"\n",
      "    operation_handlers = {\n",
      "        'sum': _calculate_sum,\n",
      "        'average': _calculate_average,\n",
      "        'max': _calculate_max\n",
      "    }\n",
      "    \n",
      "    if operation not in operation_handlers:\n",
      "        return None\n",
      "    \n",
      "    return operation_handlers[operation](data)\n",
      "\n",
      "\n",
      "def _calculate_sum(data: List[Union[int, float]]) -> Union[int, float]:\n",
      "    \"\"\"Calculates the sum of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        data (List[Union[int, float]]): A list of numbers to sum.\n",
      "\n",
      "    Returns:\n",
      "        Union[int, float]: The sum of the numbers in the list.\n",
      "    \"\"\"\n",
      "    return sum(data)\n",
      "\n",
      "\n",
      "def _calculate_average(data: List[Union[int, float]]) -> float:\n",
      "    \"\"\"Calculates the average of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        data (List[Union[int, float]]): A list of numbers to average.\n",
      "\n",
      "    Returns:\n",
      "        float: The average of the numbers, or 0.0 if the list is empty.\n",
      "    \"\"\"\n",
      "    return sum(data) / len(data) if data else 0.0\n",
      "\n",
      "\n",
      "def _calculate_max(data: List[Union[int, float]]) -> Optional[Union[int, float]]:\n",
      "    \"\"\"Finds the maximum value in a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        data (List[Union[int, float]]): A list of numbers.\n",
      "\n",
      "    Returns:\n",
      "        Optional[Union[int, float]]: The maximum value in the list, or None if\n",
      "        the list is empty.\n",
      "    \"\"\"\n",
      "    return max(data) if data else None\n"
     ]
    }
   ],
   "source": [
    "docstring_prompt = f\"\"\"\n",
    "You are a Python developer who writes excellent documentation.\n",
    "\n",
    "Add Google-style docstrings to the following Python code. Each docstring should include a description of the function, its arguments (Args:), and what it returns (Returns:).\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "{cleaned_code}\n",
    "```\n",
    "\n",
    "Output the complete Python code with the added docstrings.\n",
    "\"\"\"\n",
    "\n",
    "# Enhance the prompt to improve clarity and formatting\n",
    "enhanced_docstring_prompt = prompt_enhancer(docstring_prompt)\n",
    "\n",
    "print(\"--- Generating Docstrings ---\")\n",
    "code_with_docstrings = get_completion(enhanced_docstring_prompt, doc_client, doc_model_name, doc_api_provider)\n",
    "cleaned_code_with_docstrings = clean_llm_output(code_with_docstrings, language='python')\n",
    "print(cleaned_code_with_docstrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating a Project README\n",
    "\n",
    "**Explanation:**\n",
    "Using an LLM to generate docstrings and a README is a massive productivity boost. It excels at this structured writing task, freeing up the developer to focus on complex logic while still ensuring the project is well-documented and easy for others to understand. This prompt is a synthesis task. We provide the LLM with both high-level requirements (the PRD) and low-level implementation details (the API source code). The LLM's job is to merge these two sources of information into a single, comprehensive `README.md` file, complete with overviews, feature lists, and practical `curl` examples derived from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:29:29,952 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Project README ---\n",
      "Example template for documenting each endpoint (replace with real routes once the FastAPI code is available):\n",
      "- Method and path: GET /health\n",
      "  - Description: Liveness check.\n",
      "  - cURL:\n",
      "    ```sh\n",
      "    curl -i http://localhost:8000/health\n",
      "    ```\n",
      "\n",
      "- Method and path: POST /onboarding\n",
      "  - Description: Create a new onboarding workflow.\n",
      "  - Body: JSON payload as defined by the Pydantic model in code.\n",
      "  - cURL:\n",
      "    ```sh\n",
      "    curl -i -X POST http://localhost:8000/onboarding \\\n",
      "      -H \"Content-Type: application/json\" \\\n",
      "      -d '{ \"example\": \"payload\" }'\n",
      "    ```\n",
      "\n",
      "Replace the above with the actual endpoints found in the FastAPI app.\n",
      "\n",
      "## Setup and Installation\n",
      "Prerequisites:\n",
      "- Python 3.11+ recommended\n",
      "- pip\n",
      "- (Optional) virtualenv or venv\n",
      "\n",
      "Steps:\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary context files\n",
    "prd_content = load_artifact(\"artifacts/day1_prd.md\")\n",
    "api_code = load_artifact(\"app/main.py\")\n",
    "\n",
    "readme_prompt = f\"\"\"\n",
    "You are a technical writer creating a README.md file for a new open-source project.\n",
    "\n",
    "Use the provided Product Requirements Document (PRD) for high-level context and the FastAPI source code for technical details.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**API Source Code:**\n",
    "<code>\n",
    "{api_code}\n",
    "</code>\n",
    "\n",
    "Generate a complete README.md file with the following sections:\n",
    "- Project Title\n",
    "- Overview (Summarize the project's purpose from the PRD)\n",
    "- Features\n",
    "- API Endpoints (List the available endpoints and provide a `curl` example for each one, including the POST request with a JSON body)\n",
    "- Setup and Installation (Provide basic instructions on how to run the FastAPI app with uvicorn)\n",
    "\"\"\"\n",
    "\n",
    "# Use prompt enhancer and the readme-specific client for synthesis\n",
    "enhanced_readme_prompt = prompt_enhancer(readme_prompt)\n",
    "\n",
    "print(\"--- Generating Project README ---\")\n",
    "if prd_content and api_code:\n",
    "    readme_content = get_completion(enhanced_readme_prompt, readme_client, readme_model_name, readme_api_provider)\n",
    "    cleaned_readme = clean_llm_output(readme_content, language='markdown')\n",
    "    print(cleaned_readme)\n",
    "    save_artifact(cleaned_readme, \"README.md\")\n",
    "else:\n",
    "    print(\"Skipping README generation because PRD or API code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to perform two of the most valuable code quality tasks: refactoring and documentation. You've seen how AI can help transform messy code into a clean, maintainable structure and how it can generate comprehensive documentation from high-level project artifacts and source code. These skills are a massive productivity multiplier for any development team.\n",
    "\n",
    "> **Key Takeaway:** LLMs excel at understanding and generating structured text, whether that structure is code or documentation. Providing a clear 'before' state (the bad code) and a clear goal (the refactoring principles) allows the AI to perform complex code transformation and documentation tasks efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
